{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting face using opencv\n",
    "def getFaceBox(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "    \n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    \n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0 , 0 , i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opencv face detector\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "# age deployment \n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "# gender deployment\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age list and gendre list\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device setting\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU device\n"
     ]
    }
   ],
   "source": [
    "# model setting accordingly to device\n",
    "if device == 'cpu':\n",
    "    ageNet.setPreferableBackend(cv2.dnn.DNN_TARGET_CPU)\n",
    "    \n",
    "    genderNet.setPreferableBackend(cv2.dnn.DNN_TARGET_CPU)\n",
    "    \n",
    "    faceNet.setPreferableBackend(cv2.dnn.DNN_TARGET_CPU)\n",
    "    print(\"Using CPU device\")\n",
    "else:\n",
    "    ageNet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    ageNet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "    genderNet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    genderNet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "    genderNet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    genderNet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "    print(\"Using GPU device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a video file or an image file or a camera stream\n",
    "cap = cv2.VideoCapture(0)  #replace 0 with the path of video file for age and gender prediciton\n",
    "padding = 20\n",
    "while True:\n",
    "    # Read frame\n",
    "\n",
    "    t = time.time()\n",
    "    hasFrame, frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "\n",
    "    frameFace, bboxes = getFaceBox(faceNet, frame)\n",
    "    if not bboxes:\n",
    "        print(\"No face Detected, Checking next frame\")\n",
    "        continue\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        # print(bbox)\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        # print(\"Gender Output : {}\".format(genderPreds))\n",
    "        print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds = ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        print(\"Age Output : {}\".format(agePreds))\n",
    "        print(\"Age : {}, conf = {:.3f}\".format(age, agePreds[0].max()))\n",
    "\n",
    "        label = \"{},{}\".format(gender, age)\n",
    "        cv2.putText(frameFace, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Age Gender Demo\", frameFace)\n",
    "        # cv.imwrite(\"age-gender-out-{}\".format(args.input),frameFace)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"time : {:.3f}\".format(time.time() - t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender : Male, conf = 1.000\n",
      "Age Output : [[2.2225309e-05 5.8275257e-04 9.8269939e-01 5.2425021e-05 1.6299812e-02\n",
      "  9.7018215e-05 2.1680613e-04 2.9518760e-05]]\n",
      "Age : (8-12), conf = 0.983\n",
      "Gender : Male, conf = 1.000\n",
      "Age Output : [[2.2122951e-01 7.0148927e-01 7.7020638e-02 5.0334183e-05 6.6446075e-05\n",
      "  8.4754574e-05 2.6727008e-05 3.2206233e-05]]\n",
      "Age : (4-6), conf = 0.701\n",
      "Gender : Female, conf = 0.992\n",
      "Age Output : [[4.8369761e-06 5.5865039e-06 4.2791725e-03 6.6573936e-03 9.8674357e-01\n",
      "  2.2955353e-03 9.4001989e-06 4.4530993e-06]]\n",
      "Age : (25-32), conf = 0.987\n"
     ]
    }
   ],
   "source": [
    "# for images\n",
    "frame = cv2.imread('sample1.jpg')\n",
    "padding = 20\n",
    "frameFace, bboxes = getFaceBox(faceNet, frame)\n",
    "if not bboxes:\n",
    "    print(\"No face Detected, Checking next frame\")\n",
    "\n",
    "for bbox in bboxes:\n",
    "    # print(bbox)\n",
    "    face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    genderNet.setInput(blob)\n",
    "    genderPreds = genderNet.forward()\n",
    "    gender = genderList[genderPreds[0].argmax()]\n",
    "    # print(\"Gender Output : {}\".format(genderPreds))\n",
    "    print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
    "\n",
    "    ageNet.setInput(blob)\n",
    "    agePreds = ageNet.forward()\n",
    "    age = ageList[agePreds[0].argmax()]\n",
    "    print(\"Age Output : {}\".format(agePreds))\n",
    "    print(\"Age : {}, conf = {:.3f}\".format(age, agePreds[0].max()))\n",
    "\n",
    "    label = \"{},{}\".format(gender, age)\n",
    "    cv2.putText(frameFace, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "cv2.imshow(\"Age Gender From Image Demo\", frameFace)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ('mini')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
